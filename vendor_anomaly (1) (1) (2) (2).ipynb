{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xTfIpy09I6yX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9IQXh0D2FI5"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "tnaOMHcAEuUS",
    "outputId": "37b6b307-22c6-41e5-de23-42d71ad266fb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_1</td>\n",
       "      <td>vendor_6</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2025-04-24 19:46:31</td>\n",
       "      <td>Review text vunlrgwqgabls dcolulppf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_1</td>\n",
       "      <td>vendor_8</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2025-05-18 19:46:31</td>\n",
       "      <td>Review text r kqbkpklmvfdraz ujwtocb tibrxefi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_1</td>\n",
       "      <td>vendor_1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2025-03-27 19:46:31</td>\n",
       "      <td>Review text grxib lfsxdmah ovqalj kjdouzfcllst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_1</td>\n",
       "      <td>vendor_8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2025-04-15 19:46:31</td>\n",
       "      <td>Review text ccfwannvycnz mhqtcnsbumeocvcnchcns...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_1</td>\n",
       "      <td>vendor_4</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2025-05-15 19:46:31</td>\n",
       "      <td>Review text dexzzlxktibf yiprwhumy uwq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id vendor_id  rating            timestamp  \\\n",
       "0  user_1  vendor_6     4.3  2025-04-24 19:46:31   \n",
       "1  user_1  vendor_8     3.9  2025-05-18 19:46:31   \n",
       "2  user_1  vendor_1     4.5  2025-03-27 19:46:31   \n",
       "3  user_1  vendor_8     5.0  2025-04-15 19:46:31   \n",
       "4  user_1  vendor_4     3.8  2025-05-15 19:46:31   \n",
       "\n",
       "                                              review  \n",
       "0                Review text vunlrgwqgabls dcolulppf  \n",
       "1      Review text r kqbkpklmvfdraz ujwtocb tibrxefi  \n",
       "2  Review text grxib lfsxdmah ovqalj kjdouzfcllst...  \n",
       "3  Review text ccfwannvycnz mhqtcnsbumeocvcnchcns...  \n",
       "4             Review text dexzzlxktibf yiprwhumy uwq  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Vendor_Ratings_Dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-qL7cd5PRE8u",
    "outputId": "27342251-51df-480c-e664-e6dc52c6174e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ X_train shape: (1624, 4)\n",
      "‚úÖ X_test shape: (406, 4)\n",
      "‚úÖ y_train shape: (1624,)\n",
      "‚úÖ y_test shape: (406,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"Vendor_Ratings_Dataset.csv\")\n",
    "\n",
    "# (Optional) Convert timestamp to datetime if needed\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Let's say you want to predict \"rating\" using the other columns\n",
    "X = df.drop(columns=['rating'])  # Features (user_id, vendor_id, review, timestamp)\n",
    "y = df['rating']                 # Target (rating)\n",
    "\n",
    "# Perform train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"‚úÖ X_train shape:\", X_train.shape)\n",
    "print(\"‚úÖ X_test shape:\", X_test.shape)\n",
    "print(\"‚úÖ y_train shape:\", y_train.shape)\n",
    "print(\"‚úÖ y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y83ruxHcJTDd",
    "outputId": "b281f2d9-f2d1-45e1-d94b-f908790df330"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® Anomalies Detected:\n",
      "   user_id   vendor_id  rating  account_age  hour\n",
      "4  user_5  vendor_123     1.0            2    13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ----------- Step 1: Sample Data (Replace this with your real data) -----------\n",
    "historical_reviews = pd.DataFrame([\n",
    "    {'user_id': 'user_1', 'vendor_id': 'vendor_123', 'rating': 4.8, 'timestamp': datetime.now() - timedelta(days=3), 'user_created': datetime.now() - timedelta(days=90)},\n",
    "    {'user_id': 'user_2', 'vendor_id': 'vendor_123', 'rating': 4.7, 'timestamp': datetime.now() - timedelta(days=2), 'user_created': datetime.now() - timedelta(days=115)},\n",
    "    {'user_id': 'user_3', 'vendor_id': 'vendor_123', 'rating': 4.9, 'timestamp': datetime.now() - timedelta(days=1), 'user_created': datetime.now() - timedelta(days=100)},\n",
    "])\n",
    "\n",
    "new_reviews = pd.DataFrame([\n",
    "    {'user_id': 'user_4', 'vendor_id': 'vendor_123', 'rating': 1.0, 'timestamp': datetime.now(), 'user_created': datetime.now() - timedelta(days=1)},\n",
    "    {'user_id': 'user_5', 'vendor_id': 'vendor_123', 'rating': 1.0, 'timestamp': datetime.now(), 'user_created': datetime.now() - timedelta(days=2)},\n",
    "    {'user_id': 'user_6', 'vendor_id': 'vendor_123', 'rating': 1.0, 'timestamp': datetime.now(), 'user_created': datetime.now() - timedelta(days=1)},\n",
    "])\n",
    "\n",
    "# ----------- Step 2: Combine and Prepare -----------\n",
    "df = pd.concat([historical_reviews, new_reviews], ignore_index=True)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['user_created'] = pd.to_datetime(df['user_created'])\n",
    "df['account_age'] = (df['timestamp'] - df['user_created']).dt.days\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "\n",
    "# Encode vendor_id\n",
    "le = LabelEncoder()\n",
    "df['vendor_encoded'] = le.fit_transform(df['vendor_id'])\n",
    "\n",
    "# ----------- Step 3: Fit Isolation Forest -----------\n",
    "features = df[['rating', 'account_age', 'hour', 'vendor_encoded']]\n",
    "iso_model = IsolationForest(n_estimators=100, contamination=0.15, random_state=42)\n",
    "df['anomaly_score'] = iso_model.fit_predict(features)\n",
    "\n",
    "# ----------- Step 4: Label Anomalies -----------\n",
    "df['is_anomaly'] = df['anomaly_score'] == -1\n",
    "\n",
    "# ----------- Step 5: Optional Output (Comment out to hide) -----------\n",
    "anomalies = df[df['is_anomaly']]\n",
    "print(\"üö® Anomalies Detected:\\n\", anomalies[['user_id', 'vendor_id', 'rating', 'account_age', 'hour']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ptgZJIxWCYK",
    "outputId": "4864c130-d605-4c39-eec1-2cad18137cca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\pc\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\pc\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Kxj-n06JAm6",
    "outputId": "f46c6dd4-ebc1-4a42-fa13-771e31926553"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Point Anomalies for vendor_3:\n",
      "\n",
      "                  hour  avg_rating  rating_count\n",
      "98 2025-07-19 13:00:00         1.0            10\n"
     ]
    }
   ],
   "source": [
    "#Detect Point Anomalies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Step 1: Generate synthetic vendor rating data\n",
    "def generate_vendor_rating_data():\n",
    "    vendors = [f'vendor_{i+1}' for i in range(5)]\n",
    "    users = [f'user_{i+1}' for i in range(50)]\n",
    "    data = []\n",
    "\n",
    "    # Generate normal ratings\n",
    "    for _ in range(500):\n",
    "        data.append({\n",
    "            'user_id': random.choice(users),\n",
    "            'vendor_id': random.choice(vendors),\n",
    "            'rating': round(np.clip(np.random.normal(4.5, 0.3), 1, 5), 1),\n",
    "            'timestamp': datetime.now() - timedelta(days=random.randint(0, 30), hours=random.randint(0, 23)),\n",
    "            'review': \"normal review\"\n",
    "        })\n",
    "\n",
    "    # Inject point anomaly: sudden cluster of 1-star ratings for vendor_3\n",
    "    anomaly_time = datetime.now().replace(minute=0, second=0, microsecond=0)\n",
    "    for _ in range(10):\n",
    "        data.append({\n",
    "            'user_id': random.choice(users),\n",
    "            'vendor_id': 'vendor_3',\n",
    "            'rating': 1.0,\n",
    "            'timestamp': anomaly_time,\n",
    "            'review': \"suspicious 1-star review\"\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Step 2: Load or generate the dataset\n",
    "df_ratings = generate_vendor_rating_data()\n",
    "df_ratings['timestamp'] = pd.to_datetime(df_ratings['timestamp'])\n",
    "\n",
    "# Step 3: Detect point anomalies for vendor_3\n",
    "def detect_point_anomalies(df, vendor_id='vendor_3', threshold_rating=2.0, min_ratings=5):\n",
    "    vendor_df = df[df['vendor_id'] == vendor_id].copy()\n",
    "    vendor_df['hour'] = vendor_df['timestamp'].dt.floor('H')\n",
    "\n",
    "    # Group by hour\n",
    "    hourly_stats = vendor_df.groupby('hour').agg(\n",
    "        avg_rating=('rating', 'mean'),\n",
    "        rating_count=('rating', 'count')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Detect anomalies: low average + many ratings\n",
    "    hourly_stats['is_point_anomaly'] = (hourly_stats['avg_rating'] < threshold_rating) & \\\n",
    "                                       (hourly_stats['rating_count'] >= min_ratings)\n",
    "\n",
    "    anomalies = hourly_stats[hourly_stats['is_point_anomaly']]\n",
    "    return anomalies\n",
    "\n",
    "# Step 4: Run detection\n",
    "anomalies_detected = detect_point_anomalies(df_ratings, vendor_id='vendor_3')\n",
    "\n",
    "# Step 5: Display results\n",
    "if not anomalies_detected.empty:\n",
    "    print(\"Detected Point Anomalies for vendor_3:\\n\")\n",
    "    print(anomalies_detected[['hour', 'avg_rating', 'rating_count']])\n",
    "else:\n",
    "    print(\"No point anomalies detected for vendor_3.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hBOUPd7DlMCW",
    "outputId": "469de365-7735-4c95-f490-c028181f242e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Contextual Anomaly Check (Weekend Rating Drop):\n",
      "Average Weekday Rating: 4.51\n",
      "Average Weekend Rating: 2.00\n",
      "‚ö†Ô∏è  Contextual Anomaly Detected: Weekend ratings are unusually low!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Step 1: Generate data with consistent ratings except on weekends\n",
    "def generate_contextual_data():\n",
    "    vendors = ['vendor_1']\n",
    "    users = [f'user_{i+1}' for i in range(100)]\n",
    "    data = []\n",
    "\n",
    "    # Generate ratings across 30 days\n",
    "    for day in range(30):\n",
    "        for _ in range(10):  # 10 ratings per day\n",
    "            date = datetime.now() - timedelta(days=day)\n",
    "            rating = np.random.normal(4.5, 0.3)\n",
    "\n",
    "            # Inject lower ratings only on weekends\n",
    "            if date.weekday() in [5, 6]:  # Saturday or Sunday\n",
    "                rating = np.random.normal(2.0, 0.5)\n",
    "\n",
    "            data.append({\n",
    "                'user_id': random.choice(users),\n",
    "                'vendor_id': 'vendor_1',\n",
    "                'rating': round(np.clip(rating, 1.0, 5.0), 1),\n",
    "                'timestamp': date\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Step 2: Prepare data\n",
    "df_ratings = generate_contextual_data()\n",
    "df_ratings['timestamp'] = pd.to_datetime(df_ratings['timestamp'])\n",
    "df_ratings['day_of_week'] = df_ratings['timestamp'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "df_ratings['is_weekend'] = df_ratings['day_of_week'].isin([5, 6])\n",
    "\n",
    "# Step 3: Analyze weekend vs weekday trends\n",
    "vendor_df = df_ratings[df_ratings['vendor_id'] == 'vendor_1']\n",
    "\n",
    "# Average rating on weekdays and weekends\n",
    "weekday_avg = vendor_df[~vendor_df['is_weekend']]['rating'].mean()\n",
    "weekend_avg = vendor_df[vendor_df['is_weekend']]['rating'].mean()\n",
    "\n",
    "# Step 4: Contextual anomaly detection logic\n",
    "threshold_drop = 1.0  # if weekend avg is 1.0 lower than weekday avg\n",
    "is_contextual_anomaly = (weekday_avg - weekend_avg) >= threshold_drop\n",
    "\n",
    "# Step 5: Output\n",
    "print(\"üìä Contextual Anomaly Check (Weekend Rating Drop):\")\n",
    "print(f\"Average Weekday Rating: {weekday_avg:.2f}\")\n",
    "print(f\"Average Weekend Rating: {weekend_avg:.2f}\")\n",
    "if is_contextual_anomaly:\n",
    "    print(\"‚ö†Ô∏è  Contextual Anomaly Detected: Weekend ratings are unusually low!\")\n",
    "else:\n",
    "    print(\"‚úÖ No contextual anomaly detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RuxCAchUli7h",
    "outputId": "a21200a2-d032-4de5-9788-0596299cb89a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® Collective Anomalies Detected:\n",
      "\n",
      "Vendor: vendor_1, 5-star reviews from new users: 12, Similar Comments: 45\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Step 1: Generate data\n",
    "def generate_collective_data():\n",
    "    vendors = ['vendor_1', 'vendor_2']\n",
    "    users = [f'user_{i+1}' for i in range(100)]\n",
    "    data = []\n",
    "\n",
    "    # Normal reviews\n",
    "    for _ in range(300):\n",
    "        created = datetime.now() - timedelta(days=random.randint(10, 100))\n",
    "        data.append({\n",
    "            'user_id': random.choice(users),\n",
    "            'vendor_id': random.choice(vendors),\n",
    "            'rating': round(np.clip(np.random.normal(4.0, 0.5), 1, 5), 1),\n",
    "            'timestamp': datetime.now() - timedelta(days=random.randint(1, 30)),\n",
    "            'user_created': created,\n",
    "            'review': random.choice([\"great service\", \"excellent\", \"will buy again\", \"very satisfied\"])\n",
    "        })\n",
    "\n",
    "    # Inject fake reviews: new users, same vendor, 5-stars, similar comments\n",
    "    for i in range(10):\n",
    "        user_id = f\"fake_user_{i+1}\"\n",
    "        created = datetime.now() - timedelta(days=random.randint(0, 3))  # new account\n",
    "        data.append({\n",
    "            'user_id': user_id,\n",
    "            'vendor_id': 'vendor_1',\n",
    "            'rating': 5.0,\n",
    "            'timestamp': datetime.now() - timedelta(days=1),\n",
    "            'user_created': created,\n",
    "            'review': \"awesome vendor, very good quality\"\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Step 2: Prepare data\n",
    "df = generate_collective_data()\n",
    "df['user_created'] = pd.to_datetime(df['user_created'])\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['account_age_days'] = (df['timestamp'] - df['user_created']).dt.days\n",
    "\n",
    "# Step 3: Filter for collective pattern\n",
    "# - Account age < 7 days\n",
    "# - Rating = 5\n",
    "# - Same vendor\n",
    "# - Similar review text\n",
    "new_user_reviews = df[(df['account_age_days'] < 7) & (df['rating'] == 5.0)]\n",
    "\n",
    "# Group by vendor\n",
    "suspicious_groups = []\n",
    "for vendor, group in new_user_reviews.groupby('vendor_id'):\n",
    "    reviews = group['review'].tolist()\n",
    "    similarity_count = 0\n",
    "    for i in range(len(reviews)):\n",
    "        for j in range(i + 1, len(reviews)):\n",
    "            sim = SequenceMatcher(None, reviews[i], reviews[j]).ratio()\n",
    "            if sim > 0.85:\n",
    "                similarity_count += 1\n",
    "    if similarity_count >= 5:  # 5+ similar pairs = suspicious\n",
    "        suspicious_groups.append({\n",
    "            'vendor_id': vendor,\n",
    "            'review_count': len(group),\n",
    "            'similar_reviews': similarity_count\n",
    "        })\n",
    "\n",
    "# Step 4: Output\n",
    "if suspicious_groups:\n",
    "    print(\"üö® Collective Anomalies Detected:\\n\")\n",
    "    for group in suspicious_groups:\n",
    "        print(f\"Vendor: {group['vendor_id']}, 5-star reviews from new users: {group['review_count']}, Similar Comments: {group['similar_reviews']}\")\n",
    "else:\n",
    "    print(\"‚úÖ No collective anomalies detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mysql-connector-python in c:\\users\\pc\\anaconda3\\lib\\site-packages (9.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:762: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:762: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import mysql.connector\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# ------------------ DB Connection ------------------\n",
    "import os\n",
    "import mysql.connector\n",
    "\n",
    "db = mysql.connector.connect(\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\"),\n",
    "    database=os.getenv(\"DB_NAME\")\n",
    ")\n",
    "cursor = db.cursor()\n",
    "# ------------------ Fetch Users and Vendors ------------------\n",
    "users = pd.read_sql(\"SELECT user_id, created_at FROM users\", conn)\n",
    "vendors = pd.read_sql(\"SELECT vendor_id FROM vendor\", conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Running anomaly detection...\n",
      "‚úÖ No Point Anomaly\n",
      "\n",
      "üö® Collective Anomaly Detected: 4 new users gave same rating 5.0\n",
      "Flagged users: {8, 9, 10, 7}\n",
      "\n",
      "üìÖ Contextual Anomaly Detected: Drop from 4.67 ‚Üí 2.46\n",
      "Flagged users: {4, 5, 6, 9, 10}\n",
      "\n",
      "‚ö†Ô∏è Final flagged users (total 7): {4, 5, 6, 7, 8, 9, 10}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------- Connect to Database ----------\n",
    "import os\n",
    "import mysql.connector\n",
    "\n",
    "db = mysql.connector.connect(\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\"),\n",
    "    database=os.getenv(\"DB_NAME\")\n",
    ")\n",
    "cursor = db.cursor()\n",
    "\n",
    "# ---------- Fetch Ratings & User Info ----------\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        r.user_id,\n",
    "        r.vendor_id,\n",
    "        r.rating,\n",
    "        r.timestamp,\n",
    "        u.created_at AS account_creation_date\n",
    "    FROM reviews r\n",
    "    JOIN users u ON r.user_id = u.user_id\n",
    "\"\"\"\n",
    "cursor.execute(query)\n",
    "records = cursor.fetchall()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# ---------- Validate Column Names ----------\n",
    "expected_cols = ['user_id', 'vendor_id', 'rating', 'timestamp', 'account_creation_date']\n",
    "missing = [col for col in expected_cols if col not in df.columns]\n",
    "if missing:\n",
    "    print(f\"‚ùå Missing columns in SQL result: {missing}\")\n",
    "else:\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['account_creation_date'] = pd.to_datetime(df['account_creation_date'])\n",
    "\n",
    "    # Compute account age\n",
    "    df['account_age_days'] = (df['timestamp'] - df['account_creation_date']).dt.days\n",
    "    df['hour'] = df['timestamp'].dt.floor('H')\n",
    "\n",
    "    # ========== POINT ANOMALY ==========\n",
    "    def detect_point_anomaly(data, threshold_drop=1.5, min_ratings=4):\n",
    "        hist = data[data['timestamp'] < data['timestamp'].max().floor('H')]\n",
    "        if hist.empty:\n",
    "            return set(), None\n",
    "        baseline_avg = hist['rating'].mean()\n",
    "        grouped = data.groupby('hour').agg(avg=('rating', 'mean'), count=('rating', 'count')).reset_index()\n",
    "        anomalies = grouped[(baseline_avg - grouped['avg'] >= threshold_drop) & (grouped['count'] >= min_ratings)]\n",
    "        if anomalies.empty:\n",
    "            return set(), None\n",
    "        hours = anomalies['hour'].tolist()\n",
    "        flagged = data[data['hour'].isin(hours)]\n",
    "        return set(flagged['user_id']), anomalies\n",
    "\n",
    "    # ========== COLLECTIVE ANOMALY ==========\n",
    "    def detect_collective_anomaly(data, age_thresh=7, min_same_rating=4):\n",
    "        new_users = data[data['account_age_days'] < age_thresh]\n",
    "        if new_users.empty:\n",
    "            return set(), \"No new users\"\n",
    "        mode_rating = new_users['rating'].mode()[0]\n",
    "        suspicious = new_users[new_users['rating'] == mode_rating]\n",
    "        if len(suspicious) >= min_same_rating:\n",
    "            return set(suspicious['user_id']), f\"{len(suspicious)} new users gave same rating {mode_rating}\"\n",
    "        return set(), \"No suspicious cluster\"\n",
    "\n",
    "    # ========== CONTEXTUAL ANOMALY ==========\n",
    "    def detect_contextual_anomaly(data, drop_thresh=1.0):\n",
    "        sorted_df = data.sort_values('timestamp')\n",
    "        mid = len(sorted_df) // 2\n",
    "        past = sorted_df.iloc[:mid]\n",
    "        recent = sorted_df.iloc[mid:]\n",
    "        if len(past) < 3 or len(recent) < 3:\n",
    "            return set(), \"Not enough data\"\n",
    "        avg_past = past['rating'].mean()\n",
    "        avg_recent = recent['rating'].mean()\n",
    "        if avg_past - avg_recent >= drop_thresh:\n",
    "            return set(recent['user_id']), f\"Drop from {avg_past:.2f} ‚Üí {avg_recent:.2f}\"\n",
    "        return set(), \"No drop\"\n",
    "\n",
    "    # ========== Run All Detections ==========\n",
    "    print(\"\\nüîç Running anomaly detection...\")\n",
    "\n",
    "    all_flagged = set()\n",
    "\n",
    "    point_users, point_detail = detect_point_anomaly(df)\n",
    "    if point_users:\n",
    "        print(f\"\\nüìå Point Anomaly Detected:\\n{point_detail}\")\n",
    "        print(f\"Flagged users: {point_users}\")\n",
    "        all_flagged.update(point_users)\n",
    "    else:\n",
    "        print(\"‚úÖ No Point Anomaly\")\n",
    "\n",
    "    collective_users, coll_reason = detect_collective_anomaly(df)\n",
    "    if collective_users:\n",
    "        print(f\"\\nüö® Collective Anomaly Detected: {coll_reason}\")\n",
    "        print(f\"Flagged users: {collective_users}\")\n",
    "        all_flagged.update(collective_users)\n",
    "    else:\n",
    "        print(f\"‚úÖ No Collective Anomaly ‚Äî {coll_reason}\")\n",
    "\n",
    "    contextual_users, context_reason = detect_contextual_anomaly(df)\n",
    "    if contextual_users:\n",
    "        print(f\"\\nüìÖ Contextual Anomaly Detected: {context_reason}\")\n",
    "        print(f\"Flagged users: {contextual_users}\")\n",
    "        all_flagged.update(contextual_users)\n",
    "    else:\n",
    "        print(f\"‚úÖ No Contextual Anomaly ‚Äî {context_reason}\")\n",
    "\n",
    "    if all_flagged:\n",
    "        print(f\"\\n‚ö†Ô∏è Final flagged users (total {len(all_flagged)}): {all_flagged}\")\n",
    "    else:\n",
    "        print(\"\\nüéâ No anomalies detected in user reviews.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚õî Suspending flagged users for 24 hours...\n",
      "‚úÖ Suspended 7 users until 2025-07-20 14:32:28\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "if all_flagged:\n",
    "    print(\"\\n‚õî Suspending flagged users for 24 hours...\")\n",
    "    suspend_until = datetime.now() + timedelta(hours=24)\n",
    "\n",
    "import os\n",
    "import mysql.connector\n",
    "\n",
    "db = mysql.connector.connect(\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\"),\n",
    "    database=os.getenv(\"DB_NAME\")\n",
    ")\n",
    "    for user_id in all_flagged:\n",
    "        cursor.execute(\n",
    "            \"UPDATE users SET suspended_until = %s WHERE user_id = %s\",\n",
    "            (suspend_until, user_id)\n",
    "        )\n",
    "\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"‚úÖ Suspended {len(all_flagged)} users until {suspend_until.strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Fetching data...\n",
      "üîß Preprocessing and training model...\n",
      "‚úÖ Model trained. You can now save it separately if needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# updated_model_trainer.py\n",
    "\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Step 1: Connect to MySQL and fetch data\n",
    "def fetch_data():\n",
    "import os\n",
    "import mysql.connector\n",
    "\n",
    "db = mysql.connector.connect(\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    user=os.getenv(\"DB_USER\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\"),\n",
    "    database=os.getenv(\"DB_NAME\")\n",
    ")\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT r.user_id, r.vendor_id, r.rating, r.timestamp,\n",
    "               u.created_at AS account_creation_date\n",
    "        FROM reviews r\n",
    "        JOIN users u ON r.user_id = u.user_id\n",
    "    \"\"\")\n",
    "    data = pd.DataFrame(cursor.fetchall())\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    return data\n",
    "\n",
    "# Step 2: Preprocess data\n",
    "def preprocess_data(df):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['account_creation_date'] = pd.to_datetime(df['account_creation_date'])\n",
    "    df['account_age'] = (df['timestamp'] - df['account_creation_date']).dt.days\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    le = LabelEncoder()\n",
    "    df['vendor_encoded'] = le.fit_transform(df['vendor_id'])\n",
    "    X = df[['rating', 'account_age', 'hour', 'vendor_encoded']]\n",
    "    return X\n",
    "\n",
    "# Step 3: Build and train pipeline\n",
    "def train_pipeline(X):\n",
    "    pipeline = Pipeline([\n",
    "        ('model', IsolationForest(n_estimators=100, contamination=0.15, random_state=42))\n",
    "    ])\n",
    "    pipeline.fit(X)\n",
    "    return pipeline\n",
    "\n",
    "# Step 4: Full process to train only (no saving)\n",
    "def run():\n",
    "    print(\"üì• Fetching data...\")\n",
    "    df = fetch_data()\n",
    "    if df.empty:\n",
    "        print(\"‚ö†Ô∏è No data found in database.\")\n",
    "        return\n",
    "\n",
    "    print(\"üîß Preprocessing and training model...\")\n",
    "    X = preprocess_data(df)\n",
    "    pipeline = train_pipeline(X)\n",
    "    print(\"‚úÖ Model trained. You can now save it separately if needed.\")\n",
    "    return pipeline\n",
    "\n",
    "# Optional direct run\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ anomaly_model.pkl has been created and saved!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# 1. Create dummy vendor review data (5 rows)\n",
    "df = pd.DataFrame({\n",
    "    'rating': [4.5, 4.6, 4.7, 1.0, 1.2],\n",
    "    'account_age': [90, 80, 75, 1, 2],\n",
    "    'hour': [10, 12, 9, 8, 8],\n",
    "    'vendor_encoded': [0, 0, 0, 0, 0]\n",
    "})\n",
    "\n",
    "# 2. Build pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('model', IsolationForest(n_estimators=100, contamination=0.15, random_state=42))\n",
    "])\n",
    "\n",
    "# 3. Train the model\n",
    "pipeline.fit(df)\n",
    "\n",
    "# 4. Save it as anomaly_model.pkl in the same folder\n",
    "with open(\"anomaly_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "\n",
    "print(\"‚úÖ anomaly_model.pkl has been created and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
